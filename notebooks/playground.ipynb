{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.cli import LightningArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init\n",
    "from src.model.clip.snvl import CLIPTextEmbedding\n",
    "import src.clip as clip\n",
    "clip.tokenize([f\"1 2 3 4 5 6 7 8 9 10\", \"12313123\"]).shape\n",
    "# m = CLIPTextEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./misc/temp.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "points = torch.tensor(data['k']['lips'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_dist = torch.nn.functional.mse_loss(\n",
    "    points.unsqueeze(0), points.unsqueeze(1), reduction=\"none\"\n",
    ").flatten(1).sum(1)\n",
    "rev_dis_idx = point_dist.sort(descending=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_indices = rev_dis_idx[int(rev_dis_idx.shape[0] * 0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[target_indices].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from deepface import DeepFace\n",
    "from glob import glob\n",
    "from src.dataset.ffpp import FFPP, FFPPAugmentation, FFPPSampleStrategy\n",
    "SAVE_FOLDER = \"./misc/extern/ffpp_samples/\"\n",
    "NUM_SAMPLES = 100\n",
    "for df_type in [\"REAL\", \"DF\", \"FS\", \"NT\", \"F2F\"]:\n",
    "    dataset = FFPP(\n",
    "        df_types=[df_type],\n",
    "        compressions=[\"c23\"],\n",
    "        n_px=224,\n",
    "        strategy=FFPPSampleStrategy.NORMAL,\n",
    "        augmentations=FFPPAugmentation.NONE,\n",
    "        force_random_speed=False,\n",
    "        vid_ext=\".avi\",\n",
    "        data_dir=\"datasets/ffpp/\",\n",
    "        num_frames=1,\n",
    "        clip_duration=4,\n",
    "        transform=lambda x: x,\n",
    "        split=\"val\",\n",
    "        pack=False,\n",
    "        ratio=1.0\n",
    "    )\n",
    "\n",
    "    random.seed(1234)\n",
    "    base_dir = os.path.join(SAVE_FOLDER, df_type)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    # random sample datas\n",
    "    for i, idx in enumerate([random.randrange(0, len(dataset)) for _ in range(NUM_SAMPLES)]):\n",
    "        image = dataset.get_entity(idx)[\"clips\"]\n",
    "        image = image.flatten(0, 2).permute(1, 2, 0).numpy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #     cv2.imwrite(os.path.join(base_dir, f\"{i}.png\"), image)\n",
    "        print(\n",
    "        )\n",
    "        break\n",
    "    break\n",
    "\n",
    "    # gender_record = {}\n",
    "    # for file in glob(os.path.join(base_dir, \"*.png\")):\n",
    "    #     print(file)\n",
    "    #     file_name = os.path.splitext(os.path.split(file)[0])[0]\n",
    "    #     dom_gender = DeepFace.analyze(\n",
    "    #         img_path=file,\n",
    "    #         actions=['gender'],\n",
    "    #         enforce_detection=False\n",
    "    #     )[0][\"dominant_gender\"]\n",
    "    #     gender_record[file_name] = (0 if dom_gender == \"Woman\" else 1)\n",
    "    # gender_record_path = os.path.join(base_dir, \"gender.pickle\")\n",
    "    # with open(gender_record_path, \"wb\") as f:\n",
    "    #     pickle.dump(gender_record, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "objs = DeepFace.analyze(\n",
    "    img_path=\"test.png\",\n",
    "    actions=['gender']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clip import clip as CLIP\n",
    "CLIP.tokenize(\"woman,lady,female,\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "from deepface import DeepFace\n",
    "from notebooks.tools import load_model\n",
    "from src.clip import clip as CLIP\n",
    "from src.model.clip import FrameAttrExtractor\n",
    "from src.clip.model_vpt import PromptMode, PromptMask\n",
    "from src.dataset.ffpp import FFPP, FFPPSampleStrategy, FFPPAugmentation\n",
    "DEVICE = \"cuda\"\n",
    "NUM_SAMPLES = 100\n",
    "model, transform = CLIP.load(\"ViT-B/16\")\n",
    "model = model.float()\n",
    "\n",
    "dataset = FFPP(\n",
    "    df_types=[\"REAL\"],\n",
    "    compressions=[\"c23\"],\n",
    "    n_px=224,\n",
    "    strategy=FFPPSampleStrategy.NORMAL,\n",
    "    augmentations=FFPPAugmentation.NONE,\n",
    "    force_random_speed=False,\n",
    "    vid_ext=\".avi\",\n",
    "    data_dir=\"datasets/ffpp/\",\n",
    "    num_frames=1,\n",
    "    clip_duration=4,\n",
    "    split=\"val\",\n",
    "    transform=transform,\n",
    "    pack=False,\n",
    "    ratio=1.0\n",
    ")\n",
    "random.seed(1234)\n",
    "\n",
    "# random sample datas\n",
    "clips = []\n",
    "for idx in [random.randrange(0, len(dataset)) for _ in range(NUM_SAMPLES)]:\n",
    "    clips.append(dataset.get_entity(idx)[\"clips\"])\n",
    "clips.append(dataset.get_entity(38)[\"clips\"])\n",
    "clips = torch.cat(clips, dim=0).to(DEVICE)\n",
    "\n",
    "# detect gender\n",
    "gender_clip_idx_map = {\n",
    "    \"Woman\": [],\n",
    "    \"Man\": []\n",
    "}\n",
    "for clip_idx in range(clips.shape[0]):\n",
    "    image = clips[[clip_idx]].cpu()\n",
    "    image = image.flatten(0, 2).permute(1, 2, 0).numpy()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    gender = DeepFace.analyze(\n",
    "        img_path=image,\n",
    "        actions=['gender'],\n",
    "        enforce_detection=False,\n",
    "        silent=True\n",
    "    )[0][\"dominant_gender\"]\n",
    "    gender_clip_idx_map[gender].append(clip_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(2 * clips.shape[0], 2))\n",
    "images = clips.squeeze(1).permute(2, 0, 3, 1).flatten(1, 2).cpu().numpy()\n",
    "images = (images - images.min()) / (images.max() - images.min())\n",
    "plt.imshow(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_clip_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "with torch.no_grad():\n",
    "    text = CLIP.tokenize([\"a man\", \"a woman\"]).to(DEVICE)\n",
    "    clips = clips.squeeze(1)\n",
    "    logits_per_image, logits_per_text = model(clips, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    label = [1 if i in gender_clip_idx_map[\"Woman\"] else 0 for i in range(clips.shape[0])]\n",
    "    pred = probs[:, 1] > 0.5\n",
    "accuracy_score(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def tsne(\n",
    "    embeddings,\n",
    "    labels,\n",
    "    perplexities=[10, 50, 80],\n",
    "    graph_title=\"\",\n",
    "    save_path=\"\",\n",
    "    save=False\n",
    "):\n",
    "    X = np.array(embeddings)\n",
    "    plt.figure(figsize=(5 * len(perplexities), 4), layout=\"constrained\")\n",
    "    plt.suptitle(graph_title)\n",
    "    for i, perplexity in enumerate(perplexities):\n",
    "        print(f\"TSNE Running(Perplexity:{perplexity})....\")\n",
    "        tsne = TSNE(\n",
    "            n_components=2,\n",
    "            n_iter=10000,\n",
    "            learning_rate='auto',\n",
    "            init='random',\n",
    "            perplexity=perplexity,\n",
    "            # metric=\"cosine\"\n",
    "        )\n",
    "        _X = tsne.fit_transform(X)\n",
    "        print(f\"TSNE Completed.\")\n",
    "\n",
    "        color = {\n",
    "            \"REAL\": \"green\",\n",
    "            \"DF\": \"blue\",\n",
    "            \"FS\": \"purple\",\n",
    "            \"F2F\": \"darkorange\",\n",
    "            \"NT\": \"red\",\n",
    "            \"CDF_REAL\": \"turquoise\",\n",
    "            \"CDF_FAKE\": \"deeppink\",\n",
    "            \"DFDC_REAL\": \"forestgreen\",\n",
    "            \"DFDC_FAKE\": \"steelblue\",\n",
    "            \"Woman\": \"red\",\n",
    "            \"Man\": \"blue\"\n",
    "        }\n",
    "\n",
    "        plt.subplot(1, len(perplexities), i + 1)\n",
    "        plt.title(f\"Perplexity:{perplexity}\")\n",
    "        plt.gca().set_xticks([])\n",
    "        plt.gca().set_yticks([])\n",
    "\n",
    "        offset = 0\n",
    "        for category_type, num in labels:\n",
    "            plt.scatter(\n",
    "                _X[offset:offset + num, 0],\n",
    "                _X[offset:offset + num, 1],\n",
    "                3,\n",
    "                # color=\"green\" if category_type == \"REAL\" else \"red\",\n",
    "                color=color[category_type],\n",
    "                label=category_type if i == 0 else \"\",\n",
    "                alpha=0.5\n",
    "            )\n",
    "            offset += num\n",
    "\n",
    "        assert offset == len(_X)\n",
    "\n",
    "    plt.gcf().legend(loc='outside right center')\n",
    "    if (save):\n",
    "        folder, file = os.path.split(save_path)\n",
    "        if (not os.path.exists(folder)):\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with torch.no_grad():\n",
    "    embeds = model.encode_image(clips)\n",
    "    tsne_embeddings = []\n",
    "    tsne_labels = []\n",
    "    for gender in gender_clip_idx_map:\n",
    "        gender_embeds = embeds[gender_clip_idx_map[gender]].tolist()\n",
    "        tsne_embeddings.extend(gender_embeds)\n",
    "        tsne_labels.append((gender, len(gender_embeds)))\n",
    "    tsne(tsne_embeddings, tsne_labels, [1, 5, 10, 50, 70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx = next(i for i, data in enumerate(dataset.video_list) if data[2] == \"991\")\n",
    "entity_idx = dataset.stack_video_clips[video_idx - 1]\n",
    "clips = dataset.get_entity(entity_idx)[\"clips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = clips[0, 0].permute(1, 2, 0)\n",
    "# print(image.min(), image.max())\n",
    "# image = (image - image.min()) / (image.max() - image.min())\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "# image = Image.open(\"man.png\")\n",
    "# image = preprocess(image).permute(1, 2, 0)\n",
    "# print(image.min(), image.max())\n",
    "# image = (image - image.min()) / (image.max() - image.min())\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import clip as CLIP\n",
    "from src.clip import clip as CLIP\n",
    "from PIL import Image\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, preprocess = CLIP.load(\"ViT-B/16\", device=device)\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"man.png\")\n",
    "image = preprocess(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = CLIP.tokenize([\"a man\", \"a woman\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.99109334 0.00890664]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.clip import FrameAttrExtractor\n",
    "from src.clip.model_vpt import PromptMode\n",
    "image_embed = FrameAttrExtractor(\"ViT-B/16\", PromptMode.NONE, 0, 0, 0, True)\n",
    "image_embed = image_embed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# class TextAffinityHead(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         out_dim=2,\n",
    "#         n_prompts=10,\n",
    "#         architecture: str = \"ViT-B/16\",\n",
    "#         gender_text=[\"a man\", \"a woman\"]\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         model, _ = CLIP.load(architecture, \"cpu\")\n",
    "#         model.visual = None\n",
    "#         model = model.float().requires_grad_(False)\n",
    "#         self.out_dim = out_dim\n",
    "#         self.n_prompts = n_prompts\n",
    "#         self.ln_final = model.ln_final\n",
    "#         self.n_ctx = model.context_length\n",
    "#         self.logit_scale = model.logit_scale\n",
    "#         self.transformer = model.transformer\n",
    "#         self.text_projection = model.text_projection\n",
    "#         self.token_embedding = model.token_embedding\n",
    "#         self.positional_embedding = model.positional_embedding\n",
    "#         self.gender_text = gender_text\n",
    "\n",
    "#         # Inversion\n",
    "#         text_tokens = CLIP.tokenize(gender_text)\n",
    "#         text_embeds = self.token_embedding(text_tokens)\n",
    "#         indices = text_tokens.max(dim=1)[1]\n",
    "#         assert not False in (indices == indices[0])\n",
    "#         text_embeds = text_embeds[:, 1:indices[0]]\n",
    "#         self.gender_token = nn.Parameter(\n",
    "#             text_embeds.repeat_interleave(self.out_dim, dim=0),\n",
    "#             requires_grad=False\n",
    "#         )\n",
    "#         # generic token and prompts\n",
    "#         tokens = self.token_embedding(CLIP.tokenize(\"\"))[0]\n",
    "#         self.beg_token = nn.Parameter(\n",
    "#             tokens[0].unsqueeze(0).expand(\n",
    "#                 len(self.gender_text) * self.out_dim,\n",
    "#                 -1,\n",
    "#                 -1\n",
    "#             ),\n",
    "#             requires_grad=False\n",
    "#         )\n",
    "#         self.end_token = nn.Parameter(\n",
    "#             tokens[1].unsqueeze(0).expand(\n",
    "#                 len(self.gender_text) * self.out_dim,\n",
    "#                 -1,\n",
    "#                 -1\n",
    "#             ),\n",
    "#             requires_grad=False\n",
    "#         )\n",
    "#         self.null_token = nn.Parameter(\n",
    "#             tokens[2].unsqueeze(0).expand(\n",
    "#                 len(self.gender_text) * self.out_dim,\n",
    "#                 self.n_ctx - self.n_prompts - 2 - self.gender_token.shape[1],\n",
    "#                 -1\n",
    "#             ),\n",
    "#             requires_grad=False\n",
    "#         )\n",
    "#         self.cls_text_embed = nn.Parameter(\n",
    "#             (tokens.shape[1] ** -0.5) * torch.randn(\n",
    "#                 len(self.gender_text) * self.out_dim,\n",
    "#                 self.n_prompts,\n",
    "#                 tokens.shape[1]\n",
    "#             ),\n",
    "#             requires_grad=True\n",
    "#         )\n",
    "\n",
    "#     def create_anchors(self):\n",
    "#         x = torch.cat(\n",
    "#             (\n",
    "#                 self.beg_token,\n",
    "#                 self.gender_token,\n",
    "#                 self.cls_text_embed,\n",
    "#                 self.end_token,\n",
    "#                 self.null_token\n",
    "#             ),\n",
    "#             dim=1\n",
    "#         )  # [batch_size, n_ctx, d_model]\n",
    "#         # assert not False in (x[0] == x[1])\n",
    "#         x = x + self.positional_embedding\n",
    "#         x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "#         x = self.transformer(x)\n",
    "#         x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "#         x = self.ln_final(x)\n",
    "\n",
    "#         # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "#         # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "#         x = x[\n",
    "#             :,\n",
    "#             (self.gender_token.shape[1] + 1 + self.n_prompts)\n",
    "#         ] @ self.text_projection\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, features):\n",
    "#         anchors = self.create_anchors()\n",
    "\n",
    "#         # normalized features\n",
    "#         features = features / features.norm(dim=1, keepdim=True)\n",
    "#         anchors = anchors / anchors.norm(dim=1, keepdim=True)\n",
    "\n",
    "#         # cosine similarity as logits\n",
    "#         logit_scale = self.logit_scale.exp()\n",
    "#         logits = logit_scale * features @ anchors.t()\n",
    "#         # calculate the gender averaged probability\n",
    "#         logits = logits.view(-1, len(self.gender_text), self.out_dim).mean(dim=-1)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# text_embed = TextAffinityHead(n_prompts=1, out_dim=2)\n",
    "# text_embed = text_embed.to(device)\n",
    "# # data = text_embed.create_anchors()\n",
    "# # (data[3] - text_features[1]).abs() < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40874672, 0.5224272 , 0.06592832, 0.00289775]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    _image_features = image_embed(image)[\"embeds\"]\n",
    "    logits = text_embed(_image_features, mean_mode=\"none\")\n",
    "\n",
    "    probs = logits.softmax(dim=-1).cpu().numpy()\n",
    "probs\n",
    "# print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeroni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
